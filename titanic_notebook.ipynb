{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ce1f20-64fb-48e1-964d-47fb8af41863",
   "metadata": {},
   "source": [
    "# Titanic Logistic Regression Notebook Documentation\n",
    "\n",
    "This document details the implementation and evaluation of two logistic regression models applied to the Titanic dataset. Both models use the Newton-Raphson method to optimize the parameters; however, they differ in the preprocessing and feature engineering steps applied:\n",
    "\n",
    "- **Model 1 (Basic):** Uses basic preprocessing (simple imputation and encoding) and trains the model on the fundamental features.\n",
    "- **Model 2 (Enhanced / Feature Engineering):** Applies advanced data cleaning and feature extraction (e.g., extracting deck from Cabin, title from Name, and processing Ticket) to potentially improve the model’s predictive performance.\n",
    "---\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ef6e3-52b7-450c-918d-e09366f7c05b",
   "metadata": {},
   "source": [
    "## Table of Contents\r\n",
    "\r\n",
    "1. [Introduction](#introduction)\r\n",
    "2. [Dataset Description](#dataset-description)\r\n",
    "3. [Model 1: Basic Preprocessing and Logistic Regression](#model-1-basic-preprocessing-and-logistic-regression)\r\n",
    "    - [Basic Preprocessing](#basic-preprocessing)\r\n",
    "    - [Model 1 Implementation](#model-1-implementation)\r\n",
    "    - [Model 1 Evaluation](#model-1-evaluation)\r\n",
    "4. [Model 2: Advanced Preprocessing and Feature Engineering](#model-2-advanced-preprocessing-and-feature-engineering)\r\n",
    "    - [Advanced Preprocessing](#advanced-preprocessing)\r\n",
    "    - [Model 2 Implementation](#model-2-implementation)\r\n",
    "    - [Model 2 Evaluation](#model-2-evaluation)\r\n",
    "5. [Conclusions](#conclusions)\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcf63c4-3eab-469f-bcf1-2d8f5969aaa3",
   "metadata": {},
   "source": [
    "\r\n",
    "## 1. Introduction\r\n",
    "\r\n",
    "The purpose of this notebook is to implement a logistic regression model using the Newton-Raphson method from scratch, thereby deepening our understanding of the underlying mathematics of optimization. Two approaches are explored:\r\n",
    "\r\n",
    "- **Model 1:** Uses basic preprocessing and feature selection.\r\n",
    "- **Model 2:** Incorporates advanced preprocessing (feature engineering) by extracting additional information from variables such as *Cabin*, *Name*, and *Ticket*, which is expected to positively impact the model’s accuracy.\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddde79d4-455e-49f7-a38e-c73ba881173b",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Dataset Description\n",
    "\n",
    "The Titanic dataset is divided into two sets:\n",
    "\n",
    "- **Training set (`train.csv`):** Contains the target variable `Survived` (0 = No, 1 = Yes) along with features like `Pclass`, `Sex`, `Age`, `SibSp`, `Parch`, `Fare`, etc.\n",
    "- **Test set (`test.csv`):** Does not include the `Survived` variable and is used for generating predictions for Kaggle submission.\n",
    "\n",
    "### Key Variables\n",
    "\n",
    "- **Pclass:** Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).  \n",
    "  *(A proxy for socioeconomic status)*\n",
    "- **Sex:** Passenger gender, mapped to numerical values (0 for 'male' and 1 for 'female').\n",
    "- **Age:** Passenger age (can be fractional).\n",
    "- **SibSp / Parch:** Number of siblings/spouses and parents/children aboard.\n",
    "- **Fare:** Ticket fare.\n",
    "- **Cabin:** Cabin number (can extract deck information).\n",
    "- **Embarked:** Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaead61-c1d8-4251-9cc1-59c5404fc24f",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model 1: Basic Preprocessing and Logistic Regression\n",
    "\n",
    "### Basic Preprocessing\n",
    "\n",
    "In this approach, the following transformations are performed:\n",
    "\n",
    "- **Missing Value Imputation:**  \n",
    "  - `Age` and `Fare` are filled with their median values.\n",
    "  - `Embarked` is filled with its mode.\n",
    "- **Variable Transformation:**  \n",
    "  - Map `Sex` to numerical values.\n",
    "  - Apply one-hot encoding to the `Embarked` variable.\n",
    "- **Feature Selection:**  \n",
    "  - Select relevant features such as `Pclass`, `Age`, `SibSp`, `Parch`, `Fare`, `Sex`, and the one-hot encoded columns for `Embarked`.\n",
    "- **Intercept Term:**  \n",
    "  - Add a column of ones to represent the intercept in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5a21c9-3659-439c-9035-3eafdecc1c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "def preprocess_data(df, is_train=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Impute missing values\n",
    "    df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "    \n",
    "    if 'Embarked' in df.columns:\n",
    "        df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "    \n",
    "    # Map 'Sex' to numerical values\n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "    \n",
    "    # One-hot encoding for 'Embarked'\n",
    "    if 'Embarked' in df.columns:\n",
    "        df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
    "    \n",
    "    features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex']\n",
    "    embarked_cols = [col for col in df.columns if col.startswith('Embarked_')]\n",
    "    features += embarked_cols\n",
    "    \n",
    "    if is_train:\n",
    "        X = df[features].values.astype(np.float64)\n",
    "        y = df['Survived'].values.astype(np.float64)\n",
    "        return X, y\n",
    "    else:\n",
    "        X = df[features].values.astype(np.float64)\n",
    "        return X\n",
    "\n",
    "# Preprocess training and test sets\n",
    "X_train, y_train = preprocess_data(train_df, is_train=True)\n",
    "X_test = preprocess_data(test_df, is_train=False)\n",
    "\n",
    "# Add the intercept term\n",
    "X_train = np.hstack([np.ones((X_train.shape[0], 1), dtype=np.float64), X_train])\n",
    "X_test = np.hstack([np.ones((X_test.shape[0], 1), dtype=np.float64), X_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d340b5c-b470-44c2-85c0-b3c35248cad8",
   "metadata": {},
   "source": [
    "\n",
    "### Model 1 Implementation\n",
    "\n",
    "The logistic regression model is optimized using the Newton-Raphson method.\n",
    "\n",
    "#### Mathematical Background\n",
    "\n",
    "The **sigmoid function** is defined as:\n",
    "\n",
    "$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$\n",
    "\n",
    "The **log-likelihood** function is given by:\n",
    "\n",
    "$\n",
    "\\ell(\\beta) = \\sum \\left[ y \\cdot \\log(p) + (1-y) \\cdot \\log(1-p) \\right]\n",
    "$\n",
    "\n",
    "where $ p = \\sigma(X\\beta)$.\n",
    "\n",
    "The Newton-Raphson algorithm updates the parameters as:\n",
    "\n",
    "$\n",
    "\\beta_{\\text{new}} = \\beta - H^{-1} \\nabla \\ell(\\beta)\n",
    "$\n",
    "\n",
    "with:\n",
    "- $\\nabla \\ell(\\beta)$ as the gradient, and\n",
    "- $H$ as the Hessian matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd6aaaf2-bba8-4d96-bd57-1988a78ab31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated coefficients: [ 2.51984934e+00 -1.10076974e+00 -3.90888921e-02 -3.25369301e-01\n",
      " -9.06363873e-02  1.98291345e-03  2.72840804e+00 -6.12958075e-02\n",
      " -4.06609580e-01]\n",
      "Submission file 'submission.csv' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Log-likelihood function\n",
    "def log_likelihood(beta, X, y):\n",
    "    z = np.dot(X, beta)\n",
    "    p = sigmoid(z)\n",
    "    return np.sum(y * np.log(p + 1e-9) + (1 - y) * np.log(1 - p + 1e-9))\n",
    "\n",
    "# Gradient of the log-likelihood\n",
    "def gradient(beta, X, y):\n",
    "    z = np.dot(X, beta)\n",
    "    p = sigmoid(z)\n",
    "    return np.dot(X.T, (y - p))\n",
    "\n",
    "# Hessian matrix\n",
    "def hessian(beta, X, y):\n",
    "    z = np.dot(X, beta)\n",
    "    p = sigmoid(z)\n",
    "    W = np.diag(p * (1 - p))\n",
    "    return -np.dot(X.T, np.dot(W, X))\n",
    "\n",
    "# Initialize parameters\n",
    "beta = np.zeros(X_train.shape[1], dtype=np.float64)\n",
    "tol = 1e-24\n",
    "max_iter = 100\n",
    "\n",
    "# Newton-Raphson algorithm\n",
    "for i in range(max_iter):\n",
    "    grad = gradient(beta, X_train, y_train)\n",
    "    H = hessian(beta, X_train, y_train)\n",
    "    delta = np.linalg.solve(H, grad)\n",
    "    beta_new = beta - delta\n",
    "    if np.linalg.norm(beta_new - beta, 2) < tol:\n",
    "        beta = beta_new\n",
    "        print(f\"Convergence reached in {i+1} iterations.\")\n",
    "        break\n",
    "    beta = beta_new\n",
    "\n",
    "print(\"Estimated coefficients:\", beta)\n",
    "\n",
    "# Prediction on the test set\n",
    "z_test = np.dot(X_test, beta)\n",
    "p_test = sigmoid(z_test)\n",
    "y_pred = (p_test >= 0.5).astype(int)\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': y_pred\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file 'submission.csv' generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897ebb4d-cd26-4fe6-ba51-639a46e81bbd",
   "metadata": {},
   "source": [
    "### Model 1 Evaluation\r\n",
    "\r\n",
    "The performance on the training set is evaluated using several metrics:\r\n",
    "\r\n",
    "- **Accuracy:** The proportion of correct predictions.\r\n",
    "- **Log-Likelihood:** The value of the log-likelihood function.\r\n",
    "- **Confusion Matrix:** Provides insights into true positives, false positives, etc.\r\n",
    "- **Classification Report:** Includes precision, recall, and F1-score.\r\n",
    "- **ROC AUC Score:** Measures the model's ability to distinguish between classes.\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26d06399-7419-4756-b608-f8b22765b0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.797979797979798\n",
      "Training Log-Likelihood: -392.7616718226784\n",
      "Confusion Matrix:\n",
      "[[471  78]\n",
      " [102 240]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.86      0.84       549\n",
      "         1.0       0.75      0.70      0.73       342\n",
      "\n",
      "    accuracy                           0.80       891\n",
      "   macro avg       0.79      0.78      0.78       891\n",
      "weighted avg       0.80      0.80      0.80       891\n",
      "\n",
      "Training ROC AUC: 0.8571219335527648\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "z_train = np.dot(X_train, beta)\n",
    "p_train = sigmoid(z_train)\n",
    "y_train_pred = (p_train >= 0.5).astype(int)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.mean(y_train_pred == y_train)\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "\n",
    "# Log-likelihood\n",
    "ll_value = log_likelihood(beta, X_train, y_train)\n",
    "print(\"Training Log-Likelihood:\", ll_value)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_train, y_train_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# ROC AUC Score\n",
    "roc_auc = roc_auc_score(y_train, p_train)\n",
    "print(\"Training ROC AUC:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff24d7-7de6-46cc-83e7-f6b9e9866358",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Model 2: Advanced Preprocessing and Feature Engineering\n",
    "\n",
    "This approach enriches the dataset by extracting additional features, which may enhance the predictive performance.\n",
    "\n",
    "### Advanced Preprocessing\n",
    "\n",
    "Additional transformations include:\n",
    "\n",
    "- **Cabin:**  \n",
    "  Extract the deck letter from the Cabin (if missing, assign `'U'`).\n",
    "- **Name:**  \n",
    "  Extract the title (e.g., Mr, Mrs, Miss) from the passenger's name.\n",
    "- **Ticket:**  \n",
    "  Separate the ticket number and prefix, converting the number to numeric and creating new variables based on the prefix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ffb6086-a1f2-4488-8625-0307e4838e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: X_train = (891, 85) ; y_train = (891,) ; X_test = (418, 85)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_df(df, is_train=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Impute numerical values\n",
    "    df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "    \n",
    "    # Impute 'Embarked'\n",
    "    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "    \n",
    "    # Process Cabin: extract the deck letter\n",
    "    df['Cabin_deck'] = df['Cabin'].fillna('U').apply(lambda x: x[0] if x != 'U' else 'U')\n",
    "    \n",
    "    # Process Name: extract title\n",
    "    def extract_title(name):\n",
    "        if ',' in name:\n",
    "            parts = name.split(',')\n",
    "            if len(parts) > 1:\n",
    "                title_part = parts[1].strip()\n",
    "                title = title_part.split(' ')[0].replace('.', '')\n",
    "                return title\n",
    "        return 'None'\n",
    "    df['Title'] = df['Name'].apply(extract_title)\n",
    "    \n",
    "    # Process Ticket: extract number and prefix\n",
    "    def ticket_number(x):\n",
    "        tokens = x.split()\n",
    "        return tokens[-1] if tokens else '0'\n",
    "    def ticket_item(x):\n",
    "        tokens = x.split()\n",
    "        return \" \".join(tokens[:-1]) if len(tokens) > 1 else 'NONE'\n",
    "    df['Ticket_number'] = pd.to_numeric(df['Ticket'].apply(ticket_number), errors='coerce').fillna(0)\n",
    "    df['Ticket_item'] = df['Ticket'].apply(ticket_item)\n",
    "    \n",
    "    # Drop original columns that are no longer needed\n",
    "    df = df.drop(columns=['Name', 'Ticket', 'Cabin'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df  = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df_proc = preprocess_df(train_df, is_train=True)\n",
    "test_df_proc  = preprocess_df(test_df, is_train=False)\n",
    "\n",
    "# Define variables to use\n",
    "categorical_cols = ['Sex', 'Embarked', 'Cabin_deck', 'Title', 'Ticket_item']\n",
    "numerical_cols   = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Ticket_number']\n",
    "\n",
    "# Ensure 'Sex' is treated as a categorical variable\n",
    "train_df_proc['Sex'] = train_df_proc['Sex'].astype(str)\n",
    "test_df_proc['Sex']  = test_df_proc['Sex'].astype(str)\n",
    "\n",
    "# Concatenate for consistent encoding between train and test sets\n",
    "all_df = pd.concat([train_df_proc, test_df_proc], sort=False, ignore_index=True)\n",
    "all_df = pd.get_dummies(all_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Separate the train and test sets\n",
    "train_processed = all_df.iloc[:len(train_df_proc)].copy()\n",
    "test_processed  = all_df.iloc[len(train_df_proc):].copy()\n",
    "\n",
    "# Define target and features for training\n",
    "y_train = train_processed['Survived'].values.astype(np.float64)\n",
    "X_train = train_processed.drop(columns=['PassengerId', 'Survived']).values.astype(np.float64)\n",
    "passenger_ids = test_processed['PassengerId'].values\n",
    "X_test = test_processed.drop(columns=['PassengerId', 'Survived'], errors='ignore').values.astype(np.float64)\n",
    "\n",
    "# Add the intercept term\n",
    "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "X_test  = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "\n",
    "print(\"Shapes: X_train =\", X_train.shape, \"; y_train =\", y_train.shape, \"; X_test =\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2e6e4-7ffc-458e-9ede-f1305665d7df",
   "metadata": {},
   "source": [
    "\n",
    "### Model 2 Implementation\n",
    "\n",
    "The implementation is similar to Model 1, but here the pseudo-inverse is used for the Hessian matrix to handle possible singularities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aaa9b07-e558-4b63-ba97-38878eff51dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated coefficients: [ 5.27501290e+00 -9.23075709e-01 -3.20759269e-02 -5.29792233e-01\n",
      " -3.80086444e-01  6.04613824e-03 -1.10569050e-07 -3.44790578e+00\n",
      " -1.74320325e-01 -6.20565404e-01  3.86784739e-01 -1.15999087e-01\n",
      "  1.13306396e+00  1.98805170e+00  7.09697417e-01 -1.95845636e+00\n",
      " -1.90725974e+00 -1.88088662e-01  5.38011976e-01 -2.81828662e+00\n",
      " -1.28972557e-01 -2.24788946e-01 -3.03283985e+00  1.10077761e+00\n",
      "  3.35994555e-01  2.85768705e+00 -1.02541824e+00  6.15477129e-01\n",
      "  8.07704139e-01 -6.54154845e-01 -5.82036855e-02  1.84885376e+00\n",
      " -3.53975783e+00  4.22547128e+00  7.41029118e-01 -1.21919033e+00\n",
      " -7.22342383e-01 -1.16832422e+00 -1.59608834e+00  1.11798083e+00\n",
      " -3.50629617e-01 -5.86417193e-01 -5.59382891e-01 -1.10439104e-01\n",
      "  8.89834569e-02  2.33123297e+00  1.14874273e+00 -1.41181948e+00\n",
      " -2.87836803e+00  1.53156563e+00 -2.39397051e+00  2.36432967e+00\n",
      " -3.89921894e-01  1.41471055e-02  5.80927785e-01  8.85694791e-02\n",
      " -3.96803071e-01  2.95333682e+00 -5.91644056e-01  1.46749331e-01\n",
      " -6.81493997e+00 -2.95789396e+00 -5.42910255e-02 -8.77737923e-01\n",
      "  6.68372617e+00  1.32008893e+00 -5.76525746e-03 -4.65532471e-03\n",
      "  1.04553531e+00 -8.17692929e-01  8.19866293e-01  3.22648027e-02\n",
      " -1.21785457e+00  1.87984270e+00  2.63364324e-01 -8.65565388e-01\n",
      "  1.47632147e+00  3.32491262e+00  8.09827508e-01  1.58029801e-16\n",
      "  5.07677902e+00 -8.73849834e-01 -3.50471886e+00 -1.25337237e+00\n",
      "  1.12694432e-01]\n",
      "Log-Likelihood: -327.7630611122699\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def log_likelihood(beta, X, y):\n",
    "    z = np.dot(X, beta)\n",
    "    p = sigmoid(z)\n",
    "    return np.sum(y * np.log(p + 1e-9) + (1 - y) * np.log(1 - p + 1e-9))\n",
    "\n",
    "def gradient(beta, X, y):\n",
    "    z = np.dot(X, beta)\n",
    "    p = sigmoid(z)\n",
    "    return np.dot(X.T, (y - p))\n",
    "\n",
    "def hessian(beta, X, y):\n",
    "    z = np.dot(X, beta)\n",
    "    p = sigmoid(z)\n",
    "    W = np.diag(p * (1 - p))\n",
    "    return -np.dot(X.T, np.dot(W, X))\n",
    "\n",
    "# Initialize parameters\n",
    "beta = np.zeros(X_train.shape[1], dtype=np.float64)\n",
    "tol = 1e-12\n",
    "max_iter = 100\n",
    "\n",
    "for i in range(max_iter):\n",
    "    grad = gradient(beta, X_train, y_train)\n",
    "    H = hessian(beta, X_train, y_train)\n",
    "    # Use pseudo-inverse to handle singular Hessian matrices\n",
    "    delta = np.dot(np.linalg.pinv(H), grad)\n",
    "    beta_new = beta - delta\n",
    "    if np.linalg.norm(beta_new - beta, 2) < tol:\n",
    "        beta = beta_new\n",
    "        print(f\"Convergence reached in {i+1} iterations.\")\n",
    "        break\n",
    "    beta = beta_new\n",
    "\n",
    "print(\"Estimated coefficients:\", beta)\n",
    "print(\"Log-Likelihood:\", log_likelihood(beta, X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67faa34b-6320-4fd6-9c84-0d866e24d049",
   "metadata": {},
   "source": [
    "### Model 2 Evaluation\n",
    "\n",
    "Evaluation metrics are computed in the same manner as in Model 1, which includes accuracy, log-likelihood, confusion matrix, classification report, and ROC AUC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b685b1fc-5d95-462f-804e-88798ce64777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8451178451178452\n",
      "Confusion Matrix:\n",
      "[[484  65]\n",
      " [ 73 269]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.88      0.88       549\n",
      "         1.0       0.81      0.79      0.80       342\n",
      "\n",
      "    accuracy                           0.85       891\n",
      "   macro avg       0.84      0.83      0.84       891\n",
      "weighted avg       0.84      0.85      0.84       891\n",
      "\n",
      "Training ROC AUC: 0.9024222669606621\n",
      "Submission file 'submission.csv' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "z_train = np.dot(X_train, beta)\n",
    "p_train = sigmoid(z_train)\n",
    "y_train_pred = (p_train >= 0.5).astype(int)\n",
    "\n",
    "accuracy = np.mean(y_train_pred == y_train)\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "\n",
    "cm = confusion_matrix(y_train, y_train_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "roc_auc = roc_auc_score(y_train, p_train)\n",
    "print(\"Training ROC AUC:\", roc_auc)\n",
    "\n",
    "# Prediction on the test set and submission file generation\n",
    "z_test = np.dot(X_test, beta)\n",
    "p_test = sigmoid(z_test)\n",
    "y_test_pred = (p_test >= 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': passenger_ids,\n",
    "    'Survived': y_test_pred\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file 'submission.csv' generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a99b90-0add-4d3e-b7a9-ab1a8595192d",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e841c-5c3a-4d73-b8d9-69ad74d08e4e",
   "metadata": {},
   "source": [
    "\r\n",
    "## 5. Conclusions\r\n",
    "\r\n",
    "- **Model 1 (Basic):**  \r\n",
    "  - **Preprocessing:** Applied simple imputation (median) and basic encoding.  \r\n",
    "  - **Implementation:** Logistic regression via Newton-Raphson without advanced modifications.  \r\n",
    "  - **Results:** Reasonable performance metrics (accuracy, log-likelihood, ROC AUC), but with room for improvement.\r\n",
    "\r\n",
    "- **Model 2 (Enhanced/Feature Engineering):**  \r\n",
    "  - **Preprocessing:** Enriched the dataset by extracting information from variables like *Cabin*, *Name*, and *Ticket* and encoded multiple categorical variables consistently.  \r\n",
    "  - **Implementation:** Utilized the pseudo-inverse in the Newton-Raphson method to ensure numerical stability.  \r\n",
    "  - **Results:** Improved evaluation metrics, demonstrating that advanced preprocessing and feature engineering can positively impact the model's predictive ability.\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
